# ğŸ§  05 Naive Bayes

# ğŸ¤– Naive Bayes â€“ TL;DR + Core ML Concepts

## ğŸ“Œ What is Naive Bayes?

Naive Bayes is a **probabilistic classification algorithm** based on **Bayesâ€™ Theorem**, assuming all features are *
*independent** of each other given the class.

It predicts the **most probable class** based on observed features.

---

## ğŸ§  Bayesâ€™ Theorem (intuition)

P(Class | Features) âˆ P(Features | Class) Ã— P(Class)

Since P(Features) is constant across classes, prediction compares:
P(Features | Class) Ã— P(Class) for each class and picks the largest.

---

## âœ… Strengths

- Very fast and efficient, even on large datasets
- Performs well on **text** / **high-dimensional** data
- Requires little training data and no iterative optimization

## âš ï¸ Weaknesses

- Assumes **feature independence** (often violated)
- Struggles with **correlated** features
- Continuous features need assumptions (e.g., Gaussian) or binning

---

## ğŸ’¡ Real-world Examples

- Spam detection
- Topic/sentiment classification
- Simple medical diagnosis based on symptoms

---

## ğŸ” Most Important ML Concepts Related to Naive Bayes

### 1ï¸âƒ£ Bayesâ€™ Theorem

Combines **prior** beliefs with **evidence** from data.

### 2ï¸âƒ£ Conditional Probability

Likelihoods like P(word="offer" | spam).

### 3ï¸âƒ£ Prior & Likelihood

- **Prior**: P(Class) â€” base rate of each class.
- **Likelihood**: P(Feature | Class) â€” how typical the feature is for that class.

### 4ï¸âƒ£ Independence Assumption

Features are **conditionally independent** given the class â€” simplifies computation.

### 5ï¸âƒ£ Variants of Naive Bayes

| Variant        | Best For                | Example Use                   |
|----------------|-------------------------|-------------------------------|
| Multinomial NB | Discrete counts         | Text (word counts)            |
| Bernoulli NB   | Binary presence/absence | Spam detection                |
| Gaussian NB    | Continuous features     | Medical data (age, BMI, etc.) |

### 6ï¸âƒ£ Laplace Smoothing (Add-1)

Avoids zero probabilities for unseen featureâ€“class combinations.

### 7ï¸âƒ£ Log Probabilities

Use log-space to avoid underflow when multiplying many probabilities.

### 8ï¸âƒ£ Feature Engineering

Binning/normalization for continuous features; careful with correlated inputs.

### 9ï¸âƒ£ Scalability

Linear in number of samples Ã— features â€” extremely fast.

---

## ğŸ”— Further Reading

- ğŸ“˜ [GFG: Bayesâ€™ Theorem Explained](https://www.geeksforgeeks.org/bayes-theorem/)
- ğŸ“˜ [GFG: Naive Bayes Classifier](https://www.geeksforgeeks.org/naive-bayes-classifiers/)
