# Data Dimensionality Reduction Models (DDRM)

## ðŸ“˜ Course Overview
This repository contains theory notes, notebooks, and program-mode scripts for the master's-level course **"Data Dimensionality Reduction Models (DDRM)"**, part of the program *Modeling Big Data in Business and Finance* at Sofia University *St. Kliment Ohridski*.

The course focuses on:
- understanding **high-dimensional data** and the *curse of dimensionality*
- reducing dimensions for **visualization, compression, denoising, and modeling**
- applying classic and modern **dimensionality reduction techniques**
- evaluating trade-offs between **information retention, interpretability, and performance**

The repository is designed to be:
- âœ… exam-oriented  
- âœ… learning-oriented  
- âœ… reusable as a professional portfolio  

---

## ðŸŽ¯ Learning Objectives
By working through this repository, the goal is to:
- Understand *why* dimensionality reduction is needed in business analytics
- Distinguish **feature selection** vs **feature extraction**
- Apply linear methods (e.g., PCA/SVD) and nonlinear methods (e.g., manifold learning)
- Use DR for:
  - visualization and exploratory analysis
  - preprocessing before clustering/classification/regression
  - noise reduction and compression
- Evaluate DR methods using:
  - explained variance / reconstruction error
  - neighborhood preservation (for manifold methods)
  - downstream model performance (task-based evaluation)
- Communicate results clearly: *what was reduced, what was preserved, and what was lost*

---

## ðŸ§  Course Context & Methodology
Dimensionality reduction is not just â€œmaking data smallerâ€ â€” itâ€™s about finding **useful structure** in high-dimensional spaces.

The course is approached as a practical pipeline:
1. Understand the **data geometry** (scales, correlations, sparsity)
2. Choose a DR objective:
   - preserve variance (PCA-style)
   - preserve distances (MDS-style)
   - preserve neighborhoods (t-SNE/UMAP-style)
   - preserve information relevant to a target (supervised DR / feature selection)
3. Apply the method correctly (scaling, hyperparameters, diagnostics)
4. Evaluate both:
   - **mathematical quality** (reconstruction / structure)
   - **business usefulness** (interpretability / predictive value)
5. Document results in a reusable, lecture-by-lecture format

---

## ðŸ› ï¸ Tools & Environment
The repository uses a reproducible Python environment.

Typical libraries used:
- numpy
- pandas
- scipy
- matplotlib
- scikit-learn

Optional (when needed):
- umap-learn (UMAP)
- tensorflow / keras or pytorch (autoencoders / deep DR)

---

## ðŸ“‚ Repository Structure
```text
Data-Dimensionality-Reduction-Models/
â”‚
â”œâ”€â”€ datasets/ # Raw and processed datasets + documentation
â”‚ â”œâ”€â”€ raw/
â”‚ â”œâ”€â”€ processed/
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ notebooks/ # Jupyter notebooks for exploration & intuition
â”‚
â”œâ”€â”€ resources/ # Official lecture slides, PDFs, homework descriptions
â”‚
â”œâ”€â”€ scripts/ # Python scripts (program mode)
â”‚ â”œâ”€â”€ exercises/ # Practice scripts
â”‚ â”œâ”€â”€ homework/ # Assignment scripts
â”‚ â””â”€â”€ README.md
â”‚
â”œâ”€â”€ theory/ # Markdown notes with structured explanations
â”‚ â”œâ”€â”€ _cheatsheet.md
â”‚ â”œâ”€â”€ _glossary.md
â”‚ â””â”€â”€ README.md
â”‚
â””â”€â”€ README.md # This file
```

---

## ðŸ§ª Notebooks vs Scripts
- **Notebooks** are used to _understand_, _explain_, and _visualize_ dimensionality reduction:
  - step-by-step intuition
  - plots (2D/3D embeddings, explained variance, reconstruction error)
  - parameter sensitivity (e.g., perplexity in t-SNE)

- **Scripts** are used to _implement_, _practice_, and _reinforce_ in program mode:
  - small focused exercises (e.g., PCA from scratch)
  - homework-ready code (clean, reproducible, runnable)

Both formats are used intentionally and serve different purposes.

---

## ðŸ“ Datasets
Datasets are separated into:
- **raw/** â†’ original, immutable data
- **processed/** â†’ cleaned, scaled, encoded, or transformed data

This separation ensures reproducibility and clarity of analytical decisions.
Each dataset should be documented inside `datasets/README.md` (source, features, preprocessing, usage).

---

## ðŸ“˜ How to Use This Repository
Recommended workflow:
1. Start with **theory/** for structured understanding
2. Use **notebooks/** to see methods applied visually and step-by-step
3. Use **scripts/** to practice implementations in program mode
4. Store official materials and assignments in **resources/**
5. Track datasets and transformations in **datasets/**

This mirrors real analytics work:
**understanding â†’ experimentation â†’ implementation â†’ evaluation â†’ decision support**

---

## ðŸ“Œ Notes
- `.venv/`, `.idea/`, and notebook checkpoint files are intentionally excluded from version control
- Large datasets are not committed; instructions and references are provided instead
- The repository evolves **lecture by lecture**, with consistent structure and documentation

---

## ðŸ‘¤ Author
Graduate student in **Business Analytics / Data Analytics**, using this repository both for academic mastery and professional development.

---

> *"High dimensions hide structure; dimensionality reduction reveals what matters."*
