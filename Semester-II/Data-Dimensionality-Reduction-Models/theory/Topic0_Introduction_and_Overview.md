# Topic 0 â€“ Introduction and Overview (Lecture 1)

---

## 1ï¸âƒ£ Where DDRM Fits: AI â†’ ML â†’ Deep Learning

In this course, we study **Dimensionality Reduction** as a core tool inside **Machine Learning (ML)**, which itself is part of **Artificial Intelligence (AI)**.

- **Artificial Intelligence (AI)** = the broad goal of automating cognitive/intellectual tasks humans normally do. It includes many approaches, not only â€œlearningâ€ ones.
- **Machine Learning (ML)** = instead of hand-writing rules, we let the system **learn rules from data**.
- **Deep Learning (DL)** = a subfield of ML that learns **successive layers of representations** (typically via neural networks), often dozens or more.

**Key takeaway for DDRM:** dimensionality reduction is mostly about **finding better representations** of data-exactly the central idea behind ML (and DL).

---

## 2ï¸âƒ£ ML as a â€œNew Programming Paradigmâ€

A very practical distinction:

### ðŸ”¹ Classical programming
```text
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
Rules  â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      Classical        â”‚ â”€â”€â”€â”€â”€â”€â”€â–º  Answers
Data   â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      programming      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ðŸ”¹ Machine learning
```text
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” 
Data    â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      Machine          â”‚ â”€â”€â”€â”€â”€â”€â”€â–º  Rules
Answers â”€â”€â”€â”€â”€â”€â”€â–º  â”‚      learning         â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

This is why we say we *train* a model instead of *programming* it: the _"rules"_ are extracted from examples.

---

## 3ï¸âƒ£ What â€œLearningâ€ Means: Generalization

Training is only useful if the model can **generalize**:
- It shouldnâ€™t just perform well on the training examples
- It must perform well on **new, unseen data**

That **ability to work on unknown data** is the whole point of ML training.

---

## 4ï¸âƒ£ The Three Learning Setups (Big Picture)

### âœ… 4.1 Supervised Learning (controlled learning)
We have **labeled** data: each example includes the correct output (target).
- typical tasks: **classification** (category) and **regression** (number)

Example: The dog is guided because it has a reference (what steak smells like), just like labeled examples guide the model.

---

### âœ… 4.2 Unsupervised Learning
We have **no labels**. The goal is to find structure:
- grouping similar items (**clustering**)
- finding hidden patterns
- discovering useful representations

Example: Labels are â€œlost,â€ so the dog must group items by similarity (smell), which mirrors clustering.

Chollet connects unsupervised learning directly to **visualization, compression, denoising**, and highlights **dimensionality reduction** as a classic unsupervised category.

âž¡ï¸ **This is the main home of DDRM**: we reduce dimensions to understand data, visualize it, compress it, and remove redundancy/noise.

---

### âœ… 4.3 Reinforcement Learning (RL)
Instead of a fixed dataset, an **agent** interacts with an **environment** and learns from **rewards/penalties**.

Chollet summarizes RLwith an agent that chooses actions to maximize reward, with games as famous examples.

---

## 5ï¸âƒ£ Representations: The Bridge to Dimensionality Reduction

A powerful way to think about ML is:

> ML searches for **transformations** of data that make the task easier.

Chollet gives an intuition: a change of coordinates can turn a hard classification problem into a simple rule

These transformations can include:
- coordinate changes  
- **linear projections** (often information-reducing!)  
- nonlinear transformations  

That â€œlinear projectionâ€ idea is basically the intuition behind many dimensionality reduction methods (PCA later in the lecture).

---

## 6ï¸âƒ£ Why Deep Learning Matters (and why itâ€™s not always needed)

Deep learning became dominant not only because of performance, but because it **automates feature engineering**: instead of humans designing good features manually, a deep model learns multiple layers of representations jointly.

At the same time, the lecture warns that **most business ML today isnâ€™t necessarily deep learning**â€”sometimes you donâ€™t have enough data, or the problem is better solved with simpler methods.

**Where DDRM ties in again:** dimensionality reduction is often part of the â€œmake the data easierâ€ stepâ€”whether you later use classical ML or deep learning.

---

## 9ï¸âƒ£ Exam-Oriented Summary

- AI is the broad field; ML is learning rules from data; DL is ML that learns **layers of representations**.
- ML replaces â€œhandwritten rulesâ€ with â€œlearned rules,â€ trained from examples.
- The key ML requirement is **generalization** to unseen data.
- Three learning setups:
  - supervised = labeled targets  
  - unsupervised = no labels; structure discovery (includes **dimensionality reduction**)  
  - reinforcement = agent + environment + rewards  
- Dimensionality reduction is fundamentally about learning **better representations** (often via projections/transformations).

---

## ðŸ”‘ One-Sentence Explanation

> DDRM studies how to transform data into a smaller, more informative representation thatâ€™s easier to visualize, understand, and model.

---

## ðŸ”— References

- Lecture 1
- FranÃ§ois Chollet, *Deep Learning with Python* 
