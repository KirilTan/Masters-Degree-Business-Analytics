# Topic 1 â€“ Principal Component Analysis (PCA) (Lecture 1)

---

## 1ï¸âƒ£ Why PCA Exists (the business + ML reason)

Real datasets often have:
- **many features** (high dimensionality)
- **redundant information** (features overlap / correlate)
- **noise** that makes models less stable

A core DDRM goal is to keep the *useful structure* but work in **fewer dimensions**.

PCA is the classic answer when:
- you want **compression + visualization** (e.g., 2D/3D plots)
- you want to reduce **multicollinearity** (highly correlated predictors)
- you want a **cleaner input** for downstream models (classification/regression)

Textbook intuition: ML is essentially about finding **useful transformations / representations** of data. These transformations can include **linear projections that may destroy information**-thatâ€™s exactly what PCA is.

> Garzon summarizes PCAâ€™s intent as extracting features that retain the most **variance/covariance**, flattening data into fewer dimensions (often 2D/3D) for understanding and analysis.

---

## 2ï¸âƒ£ The Core PCA Idea in One Sentence

> **PCA finds a new coordinate system (axes) where the first axis captures the most variance, the second captures the next most, etc., and then keeps only the top axes.**

These axes are the **principal components (PCs)**.

---

## 3ï¸âƒ£ Geometric Intuition (what PCA is â€œdoingâ€)

Imagine a cloud of points in 2D/3D:
- In the original axes, the cloud might look â€œtiltedâ€
- PCA **rotates** the axes to align with the direction where the cloud spreads out the most

### âœ… PC1 (First Principal Component)
- Direction that maximizes the variance of the projected data  
- â€œWhere the data varies the mostâ€

### âœ… PC2, PC3, â€¦
- Next directions of variance
- Must be **orthogonal** (perpendicular) to previous components

So PCA is a **linear** transformation:
- rotation (and possibly scaling depending on preprocessing)
- followed by truncation (dropping some axes)

---

## 4ï¸âƒ£ The Two Main PCA Perspectives (same result, different intuition)

### 4.1 Variance Maximization
PCA chooses directions so that the **projected data keeps as much variance as possible**.  
The variance captured by each PC is linked directly to **eigenvalues** of the covariance matrix. 

### 4.2 Reconstruction Error Minimization
Another equivalent view:
- compress â†’ reconstruct  
- PCA is the best **linear** compression (in least-squares sense)

This â€œautoencoder-likeâ€ viewpoint is common in modern ML thinking:
- a linear encoder/decoder with squared loss becomes equivalent to PCA 

---

## 5ï¸âƒ£ PCA Algorithm in Practice (the steps you actually do)

The â€œstandardâ€ workflow (and what scikit-learn assumes you mean by PCA):

### âœ… Step 1: Center the data
Subtract the mean so each feature has mean 0. 

### âœ… Step 2: Standardize (usually)
Divide by standard deviation so features become unit-free and comparable. 

ðŸ“Œ Why this matters: If one feature is measured in big units (e.g., â€œincome in EURâ€) it can dominate variance and â€œhijackâ€ PCA.

### âœ… Step 3: Compute covariance matrix + eigendecomposition (or SVD)
PCA relies on eigenvectors/eigenvalues of the covariance matrix (or an SVD-based equivalent). 

### âœ… Step 4: Sort components by explained variance
Largest eigenvalue â†’ PC1, then PC2, etc.

### âœ… Step 5: Project onto the top *k* components
Your new reduced features are the coordinates in the PC basis. 

---

## 6ï¸âƒ£ Explained Variance (how many PCs should we keep?)

Each component has:
- **explained_variance_ratio** = â€œ% of total variance captured by this PCâ€

Common selection rules:
- **Scree plot**: look for the â€œelbowâ€
- **Cumulative variance threshold**: keep enough PCs to reach e.g. 90â€“95%

### âš ï¸ Important nuance
Explained variance is not the same as â€œuseful for prediction.â€
Sometimes:
- a low-variance direction can still be predictive for *y*
- PCA is unsupervised, so it doesnâ€™t â€œknowâ€ the target

---

## 7ï¸âƒ£ PCA as a Preprocessing Step in ML Pipelines

Lecture 1 demonstrates PCA combined with supervised learning through a **pipeline**:
- PCA reduces features to `n_components`
- then a classifier/regressor is trained on the reduced space
- evaluation is done with cross-validation

Example shown in the lecture: **PCA + Logistic Regression** inside a pipeline, with `n_components=10`.

ðŸ“Œ Why pipelines matter:
- scaling + PCA + model must be validated **together**
- otherwise you can get â€œdata leakageâ€ (PCA fitted using information from validation folds)

---

## 8ï¸âƒ£ How to Interpret PCA Outputs (exam-friendly)

### âœ… Principal components (PCs)
- the new axes / new features (orthogonal directions)

### âœ… Loadings
- how strongly each original feature contributes to a component
- large absolute loading â‡’ that original feature is influential for that PC

### âœ… Scores
- your data points expressed in the PC coordinate system (the transformed data)

---

## 9ï¸âƒ£ Limitations & When PCA Is a Bad Fit

PCA works best when:
- relationships are mostly **linear**
- variance is a good proxy for â€œinformationâ€

It can struggle when:
- the true structure is **nonlinear** (manifold-like)
- features are not standardized and have mismatched units
- interpretability is required in terms of original variables (PCs are mixtures)

Also note the computational aspect:
- naive eigendecomposition of a DÃ—D covariance matrix scales poorly in very high dimensions
- using SVD and low-rank approximations is the typical solution 

---

## ðŸ”Ÿ Exam-Oriented Summary

- PCA is an **unsupervised linear** dimensionality reduction method.
- It creates **orthogonal components** ordered by **explained variance**. 
- Practical steps: **center â†’ (usually) standardize â†’ eigendecompose/SVD â†’ project**. 
- Choose number of PCs using **scree plot** or **cumulative variance**. 
- PCA is commonly used in ML pipelines (Lecture: PCA + Logistic Regression example). 
- PCA optimizes variance (and equivalently minimizes reconstruction error in a linear sense). 

---

## ðŸ”‘ One-Sentence Explanation

> **PCA replaces many correlated features with a few orthogonal components that preserve as much variance (information) as possible.** 

---

## ðŸ”— References

- Lecture 1 (PCA examples + pipelines)
- Deisenroth, Faisal, Ong â€” *Mathematics for Machine Learning* (PCA steps, variance, SVD link) 
- Garzon et al. â€” *Dimensionality Reduction in Data Science* (PCA definition + intuition) 
- FranÃ§ois Chollet â€” *Deep Learning with Python* (representations + linear projections intuition) 
